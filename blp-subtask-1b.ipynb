{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Al7KNNl-FJHA"
   },
   "source": [
    "Intstalling Huggingface Transformers, Datasets, and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 98,
     "status": "ok",
     "timestamp": 1764263357857,
     "user": {
      "displayName": "Krishno Dey",
      "userId": "15212830977155173490"
     },
     "user_tz": 240
    },
    "id": "9rcTVRj21Jkv",
    "outputId": "bb276c0c-9bcd-4e9b-8032-cfc002dc3120"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39812,
     "status": "ok",
     "timestamp": 1764263398610,
     "user": {
      "displayName": "Krishno Dey",
      "userId": "15212830977155173490"
     },
     "user_tz": 240
    },
    "id": "hBGVNisxFIJE",
    "outputId": "8718fd90-f75a-437c-ef08-efc9eca4e919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.2 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2.9.0+cu126)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (1.12.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.2->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.2->transformers[torch]) (3.0.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.11.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.11.12)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.9.0+cu126)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.57.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.12.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.7.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.36.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.22.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch]\n",
    "!pip install datasets\n",
    "!pip install evaluate\n",
    "!pip install --upgrade accelerate\n",
    "!pip install --upgrade peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 782,
     "status": "ok",
     "timestamp": 1764263413980,
     "user": {
      "displayName": "Krishno Dey",
      "userId": "15212830977155173490"
     },
     "user_tz": 240
    },
    "id": "CPEes24GEa0X",
    "outputId": "8100fe83-c67e-4128-b3d8-77b4303da3da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/MyDrive/Research/NLP/Project/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gr8Gxsv3CrJL"
   },
   "source": [
    "#### Tokenize function to tokenize the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1764263416872,
     "user": {
      "displayName": "Krishno Dey",
      "userId": "15212830977155173490"
     },
     "user_tz": 240
    },
    "id": "Cc62CjYiFFkM",
    "outputId": "cf4d6b23-d839-4e57-b0a8-885a14b5e5bd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bGZvPC28H68"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def nltk_tokenization(text):\n",
    "  \"\"\"\n",
    "  text: raw text\n",
    "  return a string after tokenization\n",
    "  \"\"\"\n",
    "  return \" \".join(word_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HL_jnZ97iUeo"
   },
   "source": [
    "## tOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1764263419484,
     "user": {
      "displayName": "Krishno Dey",
      "userId": "15212830977155173490"
     },
     "user_tz": 240
    },
    "id": "I_K31s1JMG3_",
    "outputId": "1f2f05a9-2df9-4363-f8d8-dbaada9d5b76"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1764263419488,
     "user": {
      "displayName": "Krishno Dey",
      "userId": "15212830977155173490"
     },
     "user_tz": 240
    },
    "id": "updKd0C-MQcP",
    "outputId": "4dc7b9ca-9569-4c5a-caa4-1dbcef456b0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\W'\n",
      "/tmp/ipython-input-1190099680.py:3: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  tokens = re.split('\\W+', text)\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1764263419495,
     "user": {
      "displayName": "Krishno Dey",
      "userId": "15212830977155173490"
     },
     "user_tz": 240
    },
    "id": "BelVjyQsNfXY",
    "outputId": "105ae487-a1dd-43f3-8bde-59bb0a2d9555"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'gone', 'hekk', 'ddfd', 'fd']\n"
     ]
    }
   ],
   "source": [
    "t = clean_text(\"he's gone's as hekk''''''' ddfd fd\")\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffl5jWv_iY-S"
   },
   "source": [
    "## rEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9Kz16n09mv_"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def read_data(in_file, delim=','):\n",
    "  print(f\"Reading file {in_file}.......\")\n",
    "  output_data = []\n",
    "  with open(in_file, 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=delim)\n",
    "    header = next(reader)\n",
    "    output_data.append(['sentence', 'label'])\n",
    "    for row in reader:\n",
    "      if len(row) >=2:\n",
    "        tokenized_text = nltk_tokenization(row[1])\n",
    "        label = {'None': 0, 'Society': 1, 'Organization': 2, 'Community': 3, 'Individual': 4}[row[2]]\n",
    "        output_data.append([tokenized_text, label])\n",
    "  return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OckO8whoMptM"
   },
   "outputs": [],
   "source": [
    "# import csv\n",
    "# def read_data_t(in_file, delim=','):\n",
    "#   print(f\"Reading file {in_file}.......\")\n",
    "#   output_data = []\n",
    "#   with open(in_file, 'r') as f:\n",
    "#     reader = csv.reader(f, delimiter=delim)\n",
    "#     header = next(reader)\n",
    "#     output_data.append(['sentence', 'label'])\n",
    "#     for row in reader:\n",
    "#       if len(row) >=2:\n",
    "#         tokenized_text = nltk_tokenization(row[1])\n",
    "#         output_data.append([tokenized_text, 1])\n",
    "#   return output_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_ETIokcC8P9"
   },
   "source": [
    "#### Save the toeknized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OzeB4xNzC7bg"
   },
   "outputs": [],
   "source": [
    "def write_file(data_list:list, output_file, delim=','):\n",
    "  print(f\"Writing file {output_file}.......\")\n",
    "  with open(output_file, 'w') as f:\n",
    "    writer = csv.writer(f, delimiter=delim)\n",
    "    for row in data_list:\n",
    "      writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 148,
     "status": "ok",
     "timestamp": 1764263424759,
     "user": {
      "displayName": "Krishno Dey",
      "userId": "15212830977155173490"
     },
     "user_tz": 240
    },
    "id": "_QcWc8gaEbWv",
    "outputId": "70d21f55-90d3-45c0-d92b-1ec8465a6c2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data/sub-task-1b/tokenized’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data/sub-task-1b/tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5291,
     "status": "ok",
     "timestamp": 1764263430410,
     "user": {
      "displayName": "Krishno Dey",
      "userId": "15212830977155173490"
     },
     "user_tz": 240
    },
    "id": "ZaIhZ4sJD4w5",
    "outputId": "440b428f-952f-4910-dd98-d1188a3f33fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file ./data/sub-task-1b/train.tsv.......\n",
      "Writing file ./data/sub-task-1b/tokenized/train.csv.......\n",
      "Reading file ./data/sub-task-1b/dev.tsv.......\n",
      "Writing file ./data/sub-task-1b/tokenized/dev.csv.......\n",
      "Reading file ./data/sub-task-1b/test.tsv.......\n",
      "Writing file ./data/sub-task-1b/tokenized/test.csv.......\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = read_data('./data/sub-task-1b/train.tsv', delim='\\t')\n",
    "write_file(tokenized_texts, './data/sub-task-1b/tokenized/train.csv', delim=',')\n",
    "\n",
    "tokenized_texts = read_data('./data/sub-task-1b/dev.tsv', delim='\\t')\n",
    "write_file(tokenized_texts, './data/sub-task-1b/tokenized/dev.csv', delim=',')\n",
    "\n",
    "tokenized_texts = read_data('./data/sub-task-1b/test.tsv', delim='\\t')\n",
    "write_file(tokenized_texts, './data/sub-task-1b/tokenized/test.csv', delim=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1764167063825,
     "user": {
      "displayName": "Krishno Dey",
      "userId": "15212830977155173490"
     },
     "user_tz": 240
    },
    "id": "_eDA0HTC2vxL",
    "outputId": "906804b0-683e-4544-c703-4c73dc3e0f86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘output-subtask-1b’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir output-subtask-1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nah9N1rs6r_a"
   },
   "source": [
    "##rUN the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1036485,
     "status": "ok",
     "timestamp": 1764263174116,
     "user": {
      "displayName": "Krishno Dey",
      "userId": "15212830977155173490"
     },
     "user_tz": 240
    },
    "id": "VG32htcC4Avh",
    "outputId": "95d1e39e-4cd7-4705-c308-d7d302cc8add"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-27 16:49:01.247716: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764262141.279608   10342 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764262141.289490   10342 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764262141.312844   10342 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764262141.312879   10342 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764262141.312887   10342 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764262141.312893   10342 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Edited Updated v4.0 (WandB Disabled + Warning Fixed)\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=True,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.NO,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_revision=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=no,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "liger_kernel_config=None,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./output-subtask-1b/xlm-roberta-base/runs/Nov27_16-49-08_3a52fb08b447,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_TORCH_FUSED,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./output-subtask-1b/xlm-roberta-base/,\n",
      "overwrite_output_dir=True,\n",
      "parallelism_config=None,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "project=huggingface,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=None,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "trackio_space_id=trackio,\n",
      "use_cpu=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "INFO:__main__:load a local file for train: ./data/sub-task-1b/tokenized/train.csv\n",
      "INFO:__main__:load a local file for validation: ./data/sub-task-1b/tokenized/dev.csv\n",
      "INFO:__main__:load a local file for test: ./data/sub-task-1b/tokenized/test.csv\n",
      "Using custom data configuration default-f202ed4ebb33df2f\n",
      "INFO:datasets.builder:Using custom data configuration default-f202ed4ebb33df2f\n",
      "Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-f202ed4ebb33df2f/0.0.0/a43390c7ecea6519ff2ce9d10005c8750601c9e456069be5efbd2747df45f420)\n",
      "INFO:datasets.builder:Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-f202ed4ebb33df2f/0.0.0/a43390c7ecea6519ff2ce9d10005c8750601c9e456069be5efbd2747df45f420)\n",
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-f202ed4ebb33df2f/0.0.0/a43390c7ecea6519ff2ce9d10005c8750601c9e456069be5efbd2747df45f420...\n",
      "INFO:datasets.builder:Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-f202ed4ebb33df2f/0.0.0/a43390c7ecea6519ff2ce9d10005c8750601c9e456069be5efbd2747df45f420...\n",
      "Downloading took 0.0 min\n",
      "INFO:datasets.download.download_manager:Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n",
      "Generating train split\n",
      "INFO:datasets.builder:Generating train split\n",
      "Generating train split: 11840 examples [00:00, 178215.85 examples/s]\n",
      "Generating validation split\n",
      "INFO:datasets.builder:Generating validation split\n",
      "Generating validation split: 837 examples [00:00, 74096.80 examples/s]\n",
      "Generating test split\n",
      "INFO:datasets.builder:Generating test split\n",
      "Generating test split: 3400 examples [00:00, 135818.15 examples/s]\n",
      "Unable to verify splits sizes.\n",
      "INFO:datasets.utils.info_utils:Unable to verify splits sizes.\n",
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-f202ed4ebb33df2f/0.0.0/a43390c7ecea6519ff2ce9d10005c8750601c9e456069be5efbd2747df45f420. Subsequent calls will reuse this data.\n",
      "INFO:datasets.builder:Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-f202ed4ebb33df2f/0.0.0/a43390c7ecea6519ff2ce9d10005c8750601c9e456069be5efbd2747df45f420. Subsequent calls will reuse this data.\n",
      "config.json: 100% 615/615 [00:00<00:00, 5.34MB/s]\n",
      "[INFO|configuration_utils.py:765] 2025-11-27 16:49:09,004 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/config.json\n",
      "[INFO|configuration_utils.py:839] 2025-11-27 16:49:09,010 >> Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "tokenizer_config.json: 100% 25.0/25.0 [00:00<00:00, 245kB/s]\n",
      "[INFO|configuration_utils.py:765] 2025-11-27 16:49:09,184 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/config.json\n",
      "[INFO|configuration_utils.py:839] 2025-11-27 16:49:09,185 >> Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "sentencepiece.bpe.model: 100% 5.07M/5.07M [00:00<00:00, 24.1MB/s]\n",
      "tokenizer.json: 100% 9.10M/9.10M [00:00<00:00, 59.3MB/s]\n",
      "[INFO|tokenization_utils_base.py:2110] 2025-11-27 16:49:10,673 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/sentencepiece.bpe.model\n",
      "[INFO|tokenization_utils_base.py:2110] 2025-11-27 16:49:10,673 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2110] 2025-11-27 16:49:10,673 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2110] 2025-11-27 16:49:10,673 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2110] 2025-11-27 16:49:10,673 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2110] 2025-11-27 16:49:10,673 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|configuration_utils.py:765] 2025-11-27 16:49:10,674 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/config.json\n",
      "[INFO|configuration_utils.py:839] 2025-11-27 16:49:10,675 >> Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "model.safetensors: 100% 1.12G/1.12G [00:11<00:00, 94.5MB/s]\n",
      "[INFO|modeling_utils.py:1172] 2025-11-27 16:49:23,966 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/model.safetensors\n",
      "[INFO|modeling_utils.py:5525] 2025-11-27 16:49:24,020 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:5535] 2025-11-27 16:49:24,021 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset:   0% 0/11840 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-f202ed4ebb33df2f/0.0.0/a43390c7ecea6519ff2ce9d10005c8750601c9e456069be5efbd2747df45f420/cache-830279a48aaa18e6.arrow\n",
      "INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-f202ed4ebb33df2f/0.0.0/a43390c7ecea6519ff2ce9d10005c8750601c9e456069be5efbd2747df45f420/cache-830279a48aaa18e6.arrow\n",
      "Running tokenizer on dataset: 100% 11840/11840 [00:02<00:00, 4463.25 examples/s]\n",
      "Running tokenizer on dataset:   0% 0/837 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-f202ed4ebb33df2f/0.0.0/a43390c7ecea6519ff2ce9d10005c8750601c9e456069be5efbd2747df45f420/cache-7fac20b3079c653d.arrow\n",
      "INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-f202ed4ebb33df2f/0.0.0/a43390c7ecea6519ff2ce9d10005c8750601c9e456069be5efbd2747df45f420/cache-7fac20b3079c653d.arrow\n",
      "Running tokenizer on dataset: 100% 837/837 [00:00<00:00, 3917.11 examples/s]\n",
      "Running tokenizer on dataset:   0% 0/3400 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-f202ed4ebb33df2f/0.0.0/a43390c7ecea6519ff2ce9d10005c8750601c9e456069be5efbd2747df45f420/cache-c5b04bd7aad9a753.arrow\n",
      "INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-f202ed4ebb33df2f/0.0.0/a43390c7ecea6519ff2ce9d10005c8750601c9e456069be5efbd2747df45f420/cache-c5b04bd7aad9a753.arrow\n",
      "Running tokenizer on dataset: 100% 3400/3400 [00:00<00:00, 4570.22 examples/s]\n",
      "INFO:__main__:Sample 10476 of the training set: {'sentence': 'কখন লাগবে বিশ্ব যুদ্ধ অপেক্ষায় আছি', 'label': 0, 'input_ids': [0, 9755, 90645, 50395, 8751, 16550, 53264, 152585, 205247, 2801, 4979, 38665, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "INFO:__main__:Sample 1824 of the training set: {'sentence': 'এই লোকের ধৈর্য ধরে কথা শুনে কে আমি অবাক হই শুধু ডিজিটাল বললে হবে নেতাদেরও ডিজিটাল হতে হবে এমন স্লো কথা নেতা চলে না', 'label': 4, 'input_ids': [0, 6386, 87147, 896, 36147, 54546, 999, 10413, 83661, 20559, 81513, 15709, 31527, 21145, 38959, 137439, 20134, 2730, 124805, 192262, 41894, 9618, 9591, 57871, 5521, 4876, 192262, 34545, 9591, 58851, 54454, 23878, 20559, 57871, 114899, 4480, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "INFO:__main__:Sample 409 of the training set: {'sentence': 'জন্মভূমি আমার দ্বিতীয় মা', 'label': 0, 'input_ids': [0, 111927, 180998, 29388, 132922, 14767, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "[INFO|trainer.py:1012] 2025-11-27 16:49:29,381 >> The following columns in the Training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2519] 2025-11-27 16:49:29,399 >> ***** Running training *****\n",
      "[INFO|trainer.py:2520] 2025-11-27 16:49:29,399 >>   Num examples = 11,840\n",
      "[INFO|trainer.py:2521] 2025-11-27 16:49:29,399 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2522] 2025-11-27 16:49:29,399 >>   Instantaneous batch size per device = 16\n",
      "[INFO|trainer.py:2525] 2025-11-27 16:49:29,399 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2526] 2025-11-27 16:49:29,399 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2527] 2025-11-27 16:49:29,399 >>   Total optimization steps = 2,220\n",
      "[INFO|trainer.py:2528] 2025-11-27 16:49:29,399 >>   Number of trainable parameters = 278,047,493\n",
      "{'loss': 1.1181, 'grad_norm': 18.34551429748535, 'learning_rate': 1.5504504504504504e-05, 'epoch': 0.68}\n",
      " 33% 740/2220 [04:45<09:30,  2.59it/s][INFO|trainer.py:4309] 2025-11-27 16:54:15,078 >> Saving model checkpoint to ./output-subtask-1b/xlm-roberta-base/checkpoint-740\n",
      "[INFO|configuration_utils.py:491] 2025-11-27 16:54:15,085 >> Configuration saved in ./output-subtask-1b/xlm-roberta-base/checkpoint-740/config.json\n",
      "[INFO|modeling_utils.py:4181] 2025-11-27 16:54:22,654 >> Model weights saved in ./output-subtask-1b/xlm-roberta-base/checkpoint-740/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2674] 2025-11-27 16:54:22,665 >> tokenizer config file saved in ./output-subtask-1b/xlm-roberta-base/checkpoint-740/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2683] 2025-11-27 16:54:22,673 >> Special tokens file saved in ./output-subtask-1b/xlm-roberta-base/checkpoint-740/special_tokens_map.json\n",
      "{'loss': 0.8895, 'grad_norm': 26.912195205688477, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.35}\n",
      " 67% 1480/2220 [09:55<04:44,  2.60it/s][INFO|trainer.py:4309] 2025-11-27 16:59:25,031 >> Saving model checkpoint to ./output-subtask-1b/xlm-roberta-base/checkpoint-1480\n",
      "[INFO|configuration_utils.py:491] 2025-11-27 16:59:25,044 >> Configuration saved in ./output-subtask-1b/xlm-roberta-base/checkpoint-1480/config.json\n",
      "[INFO|modeling_utils.py:4181] 2025-11-27 16:59:34,284 >> Model weights saved in ./output-subtask-1b/xlm-roberta-base/checkpoint-1480/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2674] 2025-11-27 16:59:34,293 >> tokenizer config file saved in ./output-subtask-1b/xlm-roberta-base/checkpoint-1480/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2683] 2025-11-27 16:59:34,301 >> Special tokens file saved in ./output-subtask-1b/xlm-roberta-base/checkpoint-1480/special_tokens_map.json\n",
      "{'loss': 0.84, 'grad_norm': 13.869025230407715, 'learning_rate': 6.495495495495497e-06, 'epoch': 2.03}\n",
      "{'loss': 0.7385, 'grad_norm': 18.598508834838867, 'learning_rate': 1.990990990990991e-06, 'epoch': 2.7}\n",
      "100% 2220/2220 [15:03<00:00,  2.58it/s][INFO|trainer.py:4309] 2025-11-27 17:04:32,594 >> Saving model checkpoint to ./output-subtask-1b/xlm-roberta-base/checkpoint-2220\n",
      "[INFO|configuration_utils.py:491] 2025-11-27 17:04:32,622 >> Configuration saved in ./output-subtask-1b/xlm-roberta-base/checkpoint-2220/config.json\n",
      "[INFO|modeling_utils.py:4181] 2025-11-27 17:04:40,000 >> Model weights saved in ./output-subtask-1b/xlm-roberta-base/checkpoint-2220/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2674] 2025-11-27 17:04:40,034 >> tokenizer config file saved in ./output-subtask-1b/xlm-roberta-base/checkpoint-2220/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2683] 2025-11-27 17:04:40,047 >> Special tokens file saved in ./output-subtask-1b/xlm-roberta-base/checkpoint-2220/special_tokens_map.json\n",
      "[INFO|trainer.py:4418] 2025-11-27 17:04:59,569 >> Deleting older checkpoint [output-subtask-1b/xlm-roberta-base/checkpoint-740] due to args.save_total_limit\n",
      "[INFO|trainer.py:2810] 2025-11-27 17:04:59,624 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 930.2284, 'train_samples_per_second': 38.184, 'train_steps_per_second': 2.387, 'train_loss': 0.8818546673199078, 'epoch': 3.0}\n",
      "100% 2220/2220 [15:30<00:00,  2.39it/s]\n",
      "[INFO|trainer.py:4309] 2025-11-27 17:04:59,705 >> Saving model checkpoint to ./output-subtask-1b/xlm-roberta-base/\n",
      "[INFO|configuration_utils.py:491] 2025-11-27 17:04:59,729 >> Configuration saved in ./output-subtask-1b/xlm-roberta-base/config.json\n",
      "[INFO|modeling_utils.py:4181] 2025-11-27 17:05:09,520 >> Model weights saved in ./output-subtask-1b/xlm-roberta-base/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2674] 2025-11-27 17:05:09,540 >> tokenizer config file saved in ./output-subtask-1b/xlm-roberta-base/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2683] 2025-11-27 17:05:09,554 >> Special tokens file saved in ./output-subtask-1b/xlm-roberta-base/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               =  2176025GF\n",
      "  train_loss               =     0.8819\n",
      "  train_runtime            = 0:15:30.22\n",
      "  train_samples            =      11840\n",
      "  train_samples_per_second =     38.184\n",
      "  train_steps_per_second   =      2.387\n",
      "INFO:__main__:*** Evaluate ***\n",
      "[INFO|trainer.py:1012] 2025-11-27 17:05:09,814 >> The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4643] 2025-11-27 17:05:09,915 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4645] 2025-11-27 17:05:09,915 >>   Num examples = 837\n",
      "[INFO|trainer.py:4648] 2025-11-27 17:05:09,915 >>   Batch size = 8\n",
      "100% 105/105 [00:27<00:00,  3.85it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_accuracy           =     0.7073\n",
      "  eval_f1                 =     0.7056\n",
      "  eval_loss               =     0.7899\n",
      "  eval_runtime            = 0:00:06.06\n",
      "  eval_samples            =        837\n",
      "  eval_samples_per_second =     138.04\n",
      "  eval_steps_per_second   =     17.317\n",
      "INFO:__main__:*** Predict ***\n",
      "[INFO|trainer.py:1012] 2025-11-27 17:05:37,548 >> The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4643] 2025-11-27 17:05:37,555 >> \n",
      "***** Running Prediction *****\n",
      "[INFO|trainer.py:4645] 2025-11-27 17:05:37,555 >>   Num examples = 3400\n",
      "[INFO|trainer.py:4648] 2025-11-27 17:05:37,555 >>   Batch size = 8\n",
      "100% 425/425 [00:22<00:00, 18.52it/s]\n",
      "INFO:__main__:***** Predict results *****\n",
      "[INFO|modelcard.py:456] 2025-11-27 17:06:05,096 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7072879330943848}, {'name': 'F1', 'type': 'f1', 'value': 0.7056440455985032}]}\n"
     ]
    }
   ],
   "source": [
    "!python scripts/run_glue_v2.py \\\n",
    "  --model_name_or_path xlm-roberta-base \\\n",
    "  --train_file ./data/sub-task-1b/tokenized/train.csv \\\n",
    "  --validation_file ./data/sub-task-1b/tokenized/dev.csv \\\n",
    "  --test_file ./data/sub-task-1b/tokenized/test.csv \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --do_predict \\\n",
    "  --max_seq_length 128 \\\n",
    "  --per_device_train_batch_size 16 \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --num_train_epochs 2 \\\n",
    "  --output_dir ./output-subtask-1b/xlm-roberta-base/ \\\n",
    "  --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kV05gaizLRUD"
   },
   "source": [
    "## Calculating Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZaQ9GubagyS"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def calculate_performance(y_true, y_pred, labels):\n",
    "    \"\"\"\n",
    "    Calculating performances of our model\n",
    "    :param y_true: actual labels in test set\n",
    "    :param y_pred: predicted labels\n",
    "    :param labels:\n",
    "    :return: accuracy, precision, recall, f1 score and classification report\n",
    "    \"\"\"\n",
    "    (acc, P, R, F1) = (0.0, 0.0, 0.0, 0.0)\n",
    "    acc = metrics.accuracy_score(y_true, y_pred)\n",
    "    P = metrics.precision_score(y_true, y_pred, average='weighted')\n",
    "    R = metrics.recall_score(y_true, y_pred, average='weighted')\n",
    "    F1 = metrics.f1_score(y_true, y_pred, average='weighted')\n",
    "    report = metrics.classification_report(y_true, y_pred, target_names=labels)\n",
    "\n",
    "    return acc * 100, P * 100, R * 100, F1 * 100, report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zi1bMng3BHKM"
   },
   "source": [
    "#### Reading files for calculate the performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0c0pnNHdtS1L"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def read_labels(filename):\n",
    "    labels = []\n",
    "\n",
    "    # Define the mapping once\n",
    "    label_map = {\n",
    "        \"0\": \"None\",\n",
    "        \"1\": \"Society\",\n",
    "        \"2\": \"Organization\",\n",
    "        \"3\": \"Community\",\n",
    "        \"4\": \"Individual\",\n",
    "    }\n",
    "    # File Handling\n",
    "    if filename.endswith('.csv'):\n",
    "        with open(filename, 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader, None)\n",
    "            for row in reader:\n",
    "                if len(row) < 2:\n",
    "                    continue\n",
    "                # Assuming the prediction is in column 2 (index 1)\n",
    "                pred_key = row[1].strip()\n",
    "                if pred_key in label_map:\n",
    "                    labels.append(label_map[pred_key])\n",
    "    else:\n",
    "        # Using encoding='utf-8' for safety\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            # Read all lines and skip the header\n",
    "            lines = f.readlines()[1:]\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line: # Skip potential empty lines (e.g., trailing newline)\n",
    "                    continue\n",
    "                parts = line.split(\"\\t\")\n",
    "                if len(parts) < 2:\n",
    "                    continue\n",
    "                lab = parts[1] # e.g., '0'\n",
    "                if lab in label_map:\n",
    "                    labels.append(label_map[lab])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZLgQdpuBQ5v"
   },
   "source": [
    "#### Performances on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_UFT4jeNXZc"
   },
   "outputs": [],
   "source": [
    "y_test_true = read_labels('./data/sub-task-1b/tokenized/test.csv')                 #gold labels\n",
    "y_test_pred = read_labels('./output-subtask-1b/output_m-bert/predict_results.txt') #change the output file for different models\n",
    "\n",
    "assert len(y_test_true) == len(y_test_pred)\n",
    "target_labels = [\"None\", \"Society\", \"Organization\", \"Community\", \"Individual\"]\n",
    "acc, precision, recall, F1, report = calculate_performance(y_test_true, y_test_pred, target_labels)\n",
    "result = str(\"{0:.4f}\".format(acc)) + \"\\t\" + str(\"{0:.4f}\".format(precision)) + \"\\t\" + str(\n",
    "    \"{0:.4f}\".format(recall)) + \"\\t\" + str(\"{0:.4f}\".format(F1)) + \"\\n\"\n",
    "\n",
    "print(\"Test set:\\t Acc\\tPrecision\\tRecall\\tF1\\n\" + result)\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "HL_jnZ97iUeo"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
