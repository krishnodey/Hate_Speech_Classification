{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Al7KNNl-FJHA"
   },
   "source": [
    "Intstalling Huggingface Transformers, Datasets, and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18963,
     "status": "ok",
     "timestamp": 1764275528944,
     "user": {
      "displayName": "Krishno Dey",
      "userId": "15212830977155173490"
     },
     "user_tz": 240
    },
    "id": "9rcTVRj21Jkv",
    "outputId": "56a81127-682a-4ab5-8ca7-f9c79256f143"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41122,
     "status": "ok",
     "timestamp": 1764339778839,
     "user": {
      "displayName": "Krishno Dey",
      "userId": "15212830977155173490"
     },
     "user_tz": 240
    },
    "id": "hBGVNisxFIJE",
    "outputId": "a621f3cf-07ed-4302-c5b3-3a77223a2a87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.2 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2.9.0+cu126)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (1.12.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.2->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.2->transformers[torch]) (3.0.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.11.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.6\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.11.12)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.9.0+cu126)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.57.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.12.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.7.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.36.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.22.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch]\n",
    "!pip install datasets\n",
    "!pip install evaluate\n",
    "!pip install --upgrade accelerate\n",
    "!pip install --upgrade peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17708,
     "status": "ok",
     "timestamp": 1764378765694,
     "user": {
      "displayName": "Krishno Dey",
      "userId": "15212830977155173490"
     },
     "user_tz": 240
    },
    "id": "CPEes24GEa0X",
    "outputId": "324adc52-cee3-4f5f-b345-c48c6942effb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/MyDrive/Research/NLP/Project/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gr8Gxsv3CrJL"
   },
   "source": [
    "#### Tokenize function to tokenize the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2864,
     "status": "ok",
     "timestamp": 1764340450903,
     "user": {
      "displayName": "Krishno Dey",
      "userId": "15212830977155173490"
     },
     "user_tz": 240
    },
    "id": "Cc62CjYiFFkM",
    "outputId": "cb19a745-593e-4a0b-a021-07f825bf3145"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bGZvPC28H68"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def nltk_tokenization(text):\n",
    "  \"\"\"\n",
    "  text: raw text\n",
    "  return a string after tokenization\n",
    "  \"\"\"\n",
    "  return \" \".join(word_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HL_jnZ97iUeo"
   },
   "source": [
    "## tOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1764340450947,
     "user": {
      "displayName": "Krishno Dey",
      "userId": "15212830977155173490"
     },
     "user_tz": 240
    },
    "id": "I_K31s1JMG3_",
    "outputId": "20c12c5c-acbf-4e27-d38b-2c8b47427ca4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1764340450955,
     "user": {
      "displayName": "Krishno Dey",
      "userId": "15212830977155173490"
     },
     "user_tz": 240
    },
    "id": "updKd0C-MQcP",
    "outputId": "32fc9d6f-ac84-467e-d96f-a9f7eadc51fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\W'\n",
      "/tmp/ipython-input-1190099680.py:3: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  tokens = re.split('\\W+', text)\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1764340450995,
     "user": {
      "displayName": "Krishno Dey",
      "userId": "15212830977155173490"
     },
     "user_tz": 240
    },
    "id": "BelVjyQsNfXY",
    "outputId": "86954dc9-b3a5-46c9-c19e-ed786d236751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'gone', 'hekk', 'ddfd', 'fd']\n"
     ]
    }
   ],
   "source": [
    "t = clean_text(\"he's gone's as hekk''''''' ddfd fd\")\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffl5jWv_iY-S"
   },
   "source": [
    "## rEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9Kz16n09mv_"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def read_data(in_file, delim=','):\n",
    "  print(f\"Reading file {in_file}.......\")\n",
    "  output_data = []\n",
    "  with open(in_file, 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=delim)\n",
    "    header = next(reader)\n",
    "    output_data.append(['sentence', 'label'])\n",
    "    for row in reader:\n",
    "      if len(row) >=2:\n",
    "        tokenized_text = nltk_tokenization(row[1])\n",
    "        label = {\"None\": 0, \"Religious Hate\": 1, \"Sexism\": 2, \"Political Hate\": 3, \"Profane\": 4, \"Abusive\": 5}[row[2]]\n",
    "        output_data.append([tokenized_text, label])\n",
    "  return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OckO8whoMptM"
   },
   "outputs": [],
   "source": [
    "# import csv\n",
    "# def read_data_t(in_file, delim=','):\n",
    "#   print(f\"Reading file {in_file}.......\")\n",
    "#   output_data = []\n",
    "#   with open(in_file, 'r') as f:\n",
    "#     reader = csv.reader(f, delimiter=delim)\n",
    "#     header = next(reader)\n",
    "#     output_data.append(['sentence', 'label'])\n",
    "#     for row in reader:\n",
    "#       if len(row) >=2:\n",
    "#         tokenized_text = nltk_tokenization(row[1])\n",
    "#         output_data.append([tokenized_text, 1])\n",
    "#   return output_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_ETIokcC8P9"
   },
   "source": [
    "#### Save the toeknized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OzeB4xNzC7bg"
   },
   "outputs": [],
   "source": [
    "def write_file(data_list:list, output_file, delim=','):\n",
    "  print(f\"Writing file {output_file}.......\")\n",
    "  with open(output_file, 'w') as f:\n",
    "    writer = csv.writer(f, delimiter=delim)\n",
    "    for row in data_list:\n",
    "      writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1764340459532,
     "user": {
      "displayName": "Krishno Dey",
      "userId": "15212830977155173490"
     },
     "user_tz": 240
    },
    "id": "_QcWc8gaEbWv",
    "outputId": "a7a12a67-a82d-4f28-c69e-5e9ca32c5e2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data/sub-task-1a/tokenized’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data/sub-task-1a/tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4822,
     "status": "ok",
     "timestamp": 1764340464679,
     "user": {
      "displayName": "Krishno Dey",
      "userId": "15212830977155173490"
     },
     "user_tz": 240
    },
    "id": "ZaIhZ4sJD4w5",
    "outputId": "a9c77d2f-93cb-4552-e1f7-d60f49f89ae9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file ./data/sub-task-1a/train.tsv.......\n",
      "Writing file ./data/sub-task-1a/tokenized/train.csv.......\n",
      "Reading file ./data/sub-task-1a/dev.tsv.......\n",
      "Writing file ./data/sub-task-1a/tokenized/dev.csv.......\n",
      "Reading file ./data/sub-task-1a/test.tsv.......\n",
      "Writing file ./data/sub-task-1a/tokenized/test.csv.......\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = read_data('./data/sub-task-1a/train.tsv', delim='\\t')\n",
    "write_file(tokenized_texts, './data/sub-task-1a/tokenized/train.csv', delim=',')\n",
    "\n",
    "tokenized_texts = read_data('./data/sub-task-1a/dev.tsv', delim='\\t')\n",
    "write_file(tokenized_texts, './data/sub-task-1a/tokenized/dev.csv', delim=',')\n",
    "\n",
    "tokenized_texts = read_data('./data/sub-task-1a/test.tsv', delim='\\t')\n",
    "write_file(tokenized_texts, './data/sub-task-1a/tokenized/test.csv', delim=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107,
     "status": "ok",
     "timestamp": 1764275626036,
     "user": {
      "displayName": "Krishno Dey",
      "userId": "15212830977155173490"
     },
     "user_tz": 240
    },
    "id": "_eDA0HTC2vxL",
    "outputId": "45d4c79d-dc12-4948-a4c4-5bad58ceb9ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘output-subtask-1a’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir output-subtask-1a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nah9N1rs6r_a"
   },
   "source": [
    "##rUN the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 326748,
     "status": "ok",
     "timestamp": 1764279829452,
     "user": {
      "displayName": "Krishno Dey",
      "userId": "15212830977155173490"
     },
     "user_tz": 240
    },
    "id": "VG32htcC4Avh",
    "outputId": "639cad6f-1856-4855-dc14-9bcbd2c4538f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-27 21:38:30.606985: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764279510.633808   17714 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764279510.642147   17714 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764279510.671250   17714 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764279510.671280   17714 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764279510.671286   17714 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764279510.671289   17714 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Edited Updated v4.0 (WandB Disabled + Warning Fixed)\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=True,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.NO,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_revision=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=no,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "liger_kernel_config=None,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./output-subtask-1a/output_distilbert-base-uncased/runs/Nov27_21-38-41_b249bab21d4a,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=OptimizerNames.ADAMW_TORCH_FUSED,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./output-subtask-1a/output_distilbert-base-uncased/,\n",
      "overwrite_output_dir=True,\n",
      "parallelism_config=None,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "project=huggingface,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=None,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "trackio_space_id=trackio,\n",
      "use_cpu=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "INFO:__main__:load a local file for train: ./data/sub-task-1a/tokenized/train.csv\n",
      "INFO:__main__:load a local file for validation: ./data/sub-task-1a/tokenized/dev.csv\n",
      "INFO:__main__:load a local file for test: ./data/sub-task-1a/tokenized/test.csv\n",
      "Using custom data configuration default-b54c1560bcddde42\n",
      "INFO:datasets.builder:Using custom data configuration default-b54c1560bcddde42\n",
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-b54c1560bcddde42/0.0.0/a43390c7ecea6519ff2ce9d10005c8750601c9e456069be5efbd2747df45f420)\n",
      "INFO:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-b54c1560bcddde42/0.0.0/a43390c7ecea6519ff2ce9d10005c8750601c9e456069be5efbd2747df45f420)\n",
      "config.json: 100% 483/483 [00:00<00:00, 3.41MB/s]\n",
      "[INFO|configuration_utils.py:765] 2025-11-27 21:38:42,893 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n",
      "[INFO|configuration_utils.py:839] 2025-11-27 21:38:42,898 >> Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 342kB/s]\n",
      "[INFO|configuration_utils.py:765] 2025-11-27 21:38:43,377 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n",
      "[INFO|configuration_utils.py:839] 2025-11-27 21:38:43,378 >> Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "vocab.txt: 100% 232k/232k [00:00<00:00, 549kB/s]\n",
      "tokenizer.json: 100% 466k/466k [00:00<00:00, 2.12MB/s]\n",
      "[INFO|tokenization_utils_base.py:2110] 2025-11-27 21:38:46,660 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:2110] 2025-11-27 21:38:46,660 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2110] 2025-11-27 21:38:46,660 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2110] 2025-11-27 21:38:46,660 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2110] 2025-11-27 21:38:46,660 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2110] 2025-11-27 21:38:46,660 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|configuration_utils.py:765] 2025-11-27 21:38:46,661 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n",
      "[INFO|configuration_utils.py:839] 2025-11-27 21:38:46,661 >> Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "model.safetensors: 100% 268M/268M [00:02<00:00, 106MB/s]\n",
      "[INFO|modeling_utils.py:1172] 2025-11-27 21:38:49,967 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n",
      "[INFO|modeling_utils.py:5525] 2025-11-27 21:38:50,010 >> Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:5535] 2025-11-27 21:38:50,010 >> Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset:   0% 0/11840 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-b54c1560bcddde42/0.0.0/a43390c7ecea6519ff2ce9d10005c8750601c9e456069be5efbd2747df45f420/cache-ff4162514528ec52.arrow\n",
      "INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-b54c1560bcddde42/0.0.0/a43390c7ecea6519ff2ce9d10005c8750601c9e456069be5efbd2747df45f420/cache-ff4162514528ec52.arrow\n",
      "Running tokenizer on dataset: 100% 11840/11840 [00:03<00:00, 3601.81 examples/s]\n",
      "Running tokenizer on dataset:   0% 0/837 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-b54c1560bcddde42/0.0.0/a43390c7ecea6519ff2ce9d10005c8750601c9e456069be5efbd2747df45f420/cache-9bea84635a46c5ba.arrow\n",
      "INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-b54c1560bcddde42/0.0.0/a43390c7ecea6519ff2ce9d10005c8750601c9e456069be5efbd2747df45f420/cache-9bea84635a46c5ba.arrow\n",
      "Running tokenizer on dataset: 100% 837/837 [00:00<00:00, 3385.30 examples/s]\n",
      "Running tokenizer on dataset:   0% 0/3400 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-b54c1560bcddde42/0.0.0/a43390c7ecea6519ff2ce9d10005c8750601c9e456069be5efbd2747df45f420/cache-c9ffc77db3e2b127.arrow\n",
      "INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-b54c1560bcddde42/0.0.0/a43390c7ecea6519ff2ce9d10005c8750601c9e456069be5efbd2747df45f420/cache-c9ffc77db3e2b127.arrow\n",
      "Running tokenizer on dataset: 100% 3400/3400 [00:00<00:00, 6424.96 examples/s]\n",
      "INFO:__main__:Sample 10476 of the training set: {'sentence': 'কখন লাগবে বিশ্ব যুদ্ধ অপেক্ষায় আছি', 'label': 3, 'input_ids': [101, 1353, 29890, 29902, 1373, 29914, 29891, 29904, 29917, 1368, 29915, 29910, 29904, 1371, 29900, 29901, 1347, 29903, 29917, 29889, 29911, 29914, 29907, 1348, 29893, 29915, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "INFO:__main__:Sample 1824 of the training set: {'sentence': 'এই লোকের ধৈর্য ধরে কথা শুনে কে আমি অবাক হই শুধু ডিজিটাল বললে হবে নেতাদেরও ডিজিটাল হতে হবে এমন স্লো কথা নেতা চলে না', 'label': 3, 'input_ids': [101, 1351, 29885, 1373, 29917, 29914, 29889, 29917, 29908, 100, 1365, 29908, 29917, 1353, 29899, 29914, 1374, 29902, 29917, 1353, 29917, 1348, 29906, 29915, 1347, 29904, 29914, 29889, 1377, 29885, 1374, 29901, 1360, 29915, 29894, 29915, 29895, 29914, 29909, 1368, 29909, 29909, 29917, 1377, 29904, 29917, 1366, 29917, 29898, 29914, 29900, 29917, 29908, 29888, 1360, 29915, 29894, 29915, 29895, 29914, 29909, 1377, 29898, 29917, 1377, 29904, 29917, 1351, 29906, 29902, 1376, 29909, 29917, 29914, 1353, 29899, 29914, 1366, 29917, 29898, 29914, 1356, 29909, 29917, 1366, 29914, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "INFO:__main__:Sample 409 of the training set: {'sentence': 'জন্মভূমি আমার দ্বিতীয় মা', 'label': 0, 'input_ids': [101, 1358, 29902, 29906, 29905, 29906, 29915, 1348, 29906, 29914, 29908, 1364, 29904, 29915, 29898, 29916, 29907, 1370, 29914, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "[INFO|trainer.py:1012] 2025-11-27 21:38:57,041 >> The following columns in the Training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2519] 2025-11-27 21:38:57,054 >> ***** Running training *****\n",
      "[INFO|trainer.py:2520] 2025-11-27 21:38:57,054 >>   Num examples = 11,840\n",
      "[INFO|trainer.py:2521] 2025-11-27 21:38:57,054 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:2522] 2025-11-27 21:38:57,054 >>   Instantaneous batch size per device = 16\n",
      "[INFO|trainer.py:2525] 2025-11-27 21:38:57,055 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2526] 2025-11-27 21:38:57,055 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2527] 2025-11-27 21:38:57,055 >>   Total optimization steps = 1,480\n",
      "[INFO|trainer.py:2528] 2025-11-27 21:38:57,055 >>   Number of trainable parameters = 66,958,086\n",
      "{'loss': 1.1734, 'grad_norm': 4.7684173583984375, 'learning_rate': 1.3256756756756756e-05, 'epoch': 0.68}\n",
      " 50% 740/1480 [02:10<02:09,  5.73it/s][INFO|trainer.py:4309] 2025-11-27 21:41:07,619 >> Saving model checkpoint to ./output-subtask-1a/output_distilbert-base-uncased/checkpoint-740\n",
      "[INFO|configuration_utils.py:491] 2025-11-27 21:41:07,630 >> Configuration saved in ./output-subtask-1a/output_distilbert-base-uncased/checkpoint-740/config.json\n",
      "[INFO|modeling_utils.py:4181] 2025-11-27 21:41:08,604 >> Model weights saved in ./output-subtask-1a/output_distilbert-base-uncased/checkpoint-740/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2674] 2025-11-27 21:41:08,611 >> tokenizer config file saved in ./output-subtask-1a/output_distilbert-base-uncased/checkpoint-740/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2683] 2025-11-27 21:41:08,621 >> Special tokens file saved in ./output-subtask-1a/output_distilbert-base-uncased/checkpoint-740/special_tokens_map.json\n",
      "{'loss': 1.0574, 'grad_norm': 7.479780197143555, 'learning_rate': 6.5000000000000004e-06, 'epoch': 1.35}\n",
      "100% 1480/1480 [04:24<00:00,  5.74it/s][INFO|trainer.py:4309] 2025-11-27 21:43:21,409 >> Saving model checkpoint to ./output-subtask-1a/output_distilbert-base-uncased/checkpoint-1480\n",
      "[INFO|configuration_utils.py:491] 2025-11-27 21:43:21,419 >> Configuration saved in ./output-subtask-1a/output_distilbert-base-uncased/checkpoint-1480/config.json\n",
      "[INFO|modeling_utils.py:4181] 2025-11-27 21:43:22,343 >> Model weights saved in ./output-subtask-1a/output_distilbert-base-uncased/checkpoint-1480/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2674] 2025-11-27 21:43:22,351 >> tokenizer config file saved in ./output-subtask-1a/output_distilbert-base-uncased/checkpoint-1480/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2683] 2025-11-27 21:43:22,390 >> Special tokens file saved in ./output-subtask-1a/output_distilbert-base-uncased/checkpoint-1480/special_tokens_map.json\n",
      "[INFO|trainer.py:2810] 2025-11-27 21:43:28,402 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 271.3471, 'train_samples_per_second': 87.268, 'train_steps_per_second': 5.454, 'train_loss': 1.0790333722088787, 'epoch': 2.0}\n",
      "100% 1480/1480 [04:31<00:00,  5.45it/s]\n",
      "[INFO|trainer.py:4309] 2025-11-27 21:43:28,409 >> Saving model checkpoint to ./output-subtask-1a/output_distilbert-base-uncased/\n",
      "[INFO|configuration_utils.py:491] 2025-11-27 21:43:28,418 >> Configuration saved in ./output-subtask-1a/output_distilbert-base-uncased/config.json\n",
      "[INFO|modeling_utils.py:4181] 2025-11-27 21:43:29,567 >> Model weights saved in ./output-subtask-1a/output_distilbert-base-uncased/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2674] 2025-11-27 21:43:29,574 >> tokenizer config file saved in ./output-subtask-1a/output_distilbert-base-uncased/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2683] 2025-11-27 21:43:29,581 >> Special tokens file saved in ./output-subtask-1a/output_distilbert-base-uncased/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  total_flos               =   730401GF\n",
      "  train_loss               =      1.079\n",
      "  train_runtime            = 0:04:31.34\n",
      "  train_samples            =      11840\n",
      "  train_samples_per_second =     87.268\n",
      "  train_steps_per_second   =      5.454\n",
      "INFO:__main__:*** Evaluate ***\n",
      "[INFO|trainer.py:1012] 2025-11-27 21:43:30,229 >> The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4643] 2025-11-27 21:43:30,242 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4645] 2025-11-27 21:43:30,242 >>   Num examples = 837\n",
      "[INFO|trainer.py:4648] 2025-11-27 21:43:30,242 >>   Batch size = 8\n",
      "100% 105/105 [00:03<00:00, 34.15it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_accuracy           =     0.6213\n",
      "  eval_f1                 =     0.5741\n",
      "  eval_loss               =     0.9599\n",
      "  eval_runtime            = 0:00:03.11\n",
      "  eval_samples            =        837\n",
      "  eval_samples_per_second =    268.671\n",
      "  eval_steps_per_second   =     33.704\n",
      "INFO:__main__:*** Predict ***\n",
      "[INFO|trainer.py:1012] 2025-11-27 21:43:33,400 >> The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4643] 2025-11-27 21:43:33,410 >> \n",
      "***** Running Prediction *****\n",
      "[INFO|trainer.py:4645] 2025-11-27 21:43:33,410 >>   Num examples = 3400\n",
      "[INFO|trainer.py:4648] 2025-11-27 21:43:33,410 >>   Batch size = 8\n",
      "100% 425/425 [00:12<00:00, 33.24it/s]\n",
      "INFO:__main__:***** Predict results *****\n",
      "[INFO|modelcard.py:456] 2025-11-27 21:43:46,750 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.6212664277180406}, {'name': 'F1', 'type': 'f1', 'value': 0.5740933064153947}]}\n"
     ]
    }
   ],
   "source": [
    "!python scripts/run_glue_v2.py \\\n",
    "  --model_name_or_path distilbert-base-uncased \\\n",
    "  --train_file ./data/sub-task-1a/tokenized/train.csv \\\n",
    "  --validation_file ./data/sub-task-1a/tokenized/dev.csv \\\n",
    "  --test_file ./data/sub-task-1a/tokenized/test.csv \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --do_predict \\\n",
    "  --max_seq_length 128 \\\n",
    "  --per_device_train_batch_size 16 \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --num_train_epochs 2 \\\n",
    "  --output_dir ./output-subtask-1a/output_distilbert-base-uncased/ \\\n",
    "  --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRLp_b1zMGE0"
   },
   "source": [
    "## Calculating Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZaQ9GubagyS"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def calculate_performance(y_true, y_pred, labels):\n",
    "    \"\"\"\n",
    "    Calculating performances of our model\n",
    "    :param y_true: actual labels in test set\n",
    "    :param y_pred: predicted labels\n",
    "    :param labels:\n",
    "    :return: accuracy, precision, recall, f1 score and classification report\n",
    "    \"\"\"\n",
    "    (acc, P, R, F1) = (0.0, 0.0, 0.0, 0.0)\n",
    "    acc = metrics.accuracy_score(y_true, y_pred)\n",
    "    P = metrics.precision_score(y_true, y_pred, average='weighted')\n",
    "    R = metrics.recall_score(y_true, y_pred, average='weighted')\n",
    "    F1 = metrics.f1_score(y_true, y_pred, average='weighted')\n",
    "    report = metrics.classification_report(y_true, y_pred, target_names=labels)\n",
    "\n",
    "    return acc * 100, P * 100, R * 100, F1 * 100, report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zi1bMng3BHKM"
   },
   "source": [
    "#### Reading files for calculate the performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4RGudJN60Ag"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def read_labels(filename):\n",
    "    labels = []\n",
    "\n",
    "    # Define the mapping once\n",
    "    label_map = {\n",
    "        \"0\": \"None\",\n",
    "        \"1\": \"Religious Hate\",\n",
    "        \"2\": \"Sexism\",\n",
    "        \"3\": \"Political Hate\",\n",
    "        \"4\": \"Profane\",\n",
    "        \"5\": \"Abusive\"\n",
    "    }\n",
    "    # File Handling\n",
    "    if filename.endswith('.csv'):\n",
    "        with open(filename, 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader, None)\n",
    "            for row in reader:\n",
    "                if len(row) < 2:\n",
    "                    continue\n",
    "                # Assuming the prediction is in column 2 (index 1)\n",
    "                pred_key = row[1].strip()\n",
    "                if pred_key in label_map:\n",
    "                    labels.append(label_map[pred_key])\n",
    "    else:\n",
    "        # Using encoding='utf-8' for safety\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            # Read all lines and skip the header\n",
    "            lines = f.readlines()[1:]\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line: # Skip potential empty lines (e.g., trailing newline)\n",
    "                    continue\n",
    "                parts = line.split(\"\\t\")\n",
    "                if len(parts) < 2:\n",
    "                    continue\n",
    "                lab = parts[1] # e.g., '0'\n",
    "                if lab in label_map:\n",
    "                    labels.append(label_map[lab])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZLgQdpuBQ5v"
   },
   "source": [
    "#### Performances on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 158,
     "status": "ok",
     "timestamp": 1764279837590,
     "user": {
      "displayName": "Krishno Dey",
      "userId": "15212830977155173490"
     },
     "user_tz": 240
    },
    "id": "qOhlPRfW7i2x",
    "outputId": "d47701d2-ca18-465b-cb08-4d6d051dfa01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\t Acc\tPrecision\tRecall\tF1\n",
      "62.0294\t56.2471\t62.0294\t56.7535\n",
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "          None       0.37      0.18      0.24       753\n",
      "Religious Hate       0.66      0.91      0.77      1939\n",
      "        Sexism       0.52      0.31      0.39       410\n",
      "Political Hate       0.60      0.36      0.45       221\n",
      "       Profane       0.00      0.00      0.00        68\n",
      "       Abusive       0.00      0.00      0.00         9\n",
      "\n",
      "      accuracy                           0.62      3400\n",
      "     macro avg       0.36      0.29      0.31      3400\n",
      "  weighted avg       0.56      0.62      0.57      3400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "y_test_true = read_labels('./data/sub-task-1a/tokenized/test.csv')                                #gold labels\n",
    "y_test_pred = read_labels('./output-subtask-1a/output_distilbert-base-cased/predict_results.txt') #change the output file for different models\n",
    "assert len(y_test_true) == len(y_test_pred)\n",
    "target_labels = [\"None\", \"Religious Hate\", \"Sexism\", \"Political Hate\", \"Profane\", \"Abusive\"]\n",
    "acc, precision, recall, F1, report = calculate_performance(y_test_true, y_test_pred, target_labels)\n",
    "result = str(\"{0:.4f}\".format(acc)) + \"\\t\" + str(\"{0:.4f}\".format(precision)) + \"\\t\" + str(\n",
    "    \"{0:.4f}\".format(recall)) + \"\\t\" + str(\"{0:.4f}\".format(F1)) + \"\\n\"\n",
    "\n",
    "print(\"Test set:\\t Acc\\tPrecision\\tRecall\\tF1\\n\" + result)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "HL_jnZ97iUeo"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
